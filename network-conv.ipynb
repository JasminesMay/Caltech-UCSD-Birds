{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "####################################### PART ONE #######################################\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import datetime\n",
    "import util\n",
    "\n",
    "NUM_CLASSES = 200\n",
    "LIMIT = 75\n",
    "NUM_CLASSES = LIMIT\n",
    "\n",
    "IMAGES_MEAN = 122.5\n",
    "IMAGES_STD = 63.32\n",
    "\n",
    "LOGITS_COLLECTION = 'LOGITS'\n",
    "LOGIT_LABELS_COLLECTION = 'LOGIT-LABELS'\n",
    "\n",
    "RUN_PREFIX = datetime.datetime.fromtimestamp(datetime.datetime.now().timestamp()).strftime('%Y-%m-%d-%H_%M_%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552\n",
      "138\n"
     ]
    }
   ],
   "source": [
    "def load(filename):\n",
    "    file = open(filename, \"r\") \n",
    "    image_names = file.readlines()\n",
    "    images = []\n",
    "    labels = []\n",
    "    for name in image_names:\n",
    "        label = int(name[:3])\n",
    "        if label <= LIMIT:\n",
    "            im = Image.open(\"images/\" + name.rstrip('\\n'))\n",
    "            H, W = im.size\n",
    "            pixels = list(im.getdata())\n",
    "            if not type(pixels[0]) is int:\n",
    "                # todo: right now we are discarding transparent images\n",
    "                image = np.array([comp for pixel in pixels for comp in pixel]).reshape(-1, H, W, 3)\n",
    "                images.append(image)\n",
    "                # zero-index the label\n",
    "                labels.append(label - 1)\n",
    "        else: \n",
    "            break\n",
    "    return images, labels\n",
    "\n",
    "images_train_and_val, labels_train_and_val = load('train.txt')\n",
    "\n",
    "seed = 13958293\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(images_train_and_val)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(labels_train_and_val)\n",
    "\n",
    "\n",
    "images_train = images_train_and_val[:int(len(images_train_and_val) * .80)]\n",
    "images_val = images_train_and_val[int(len(images_train_and_val) * .80):]\n",
    "\n",
    "labels_train = labels_train_and_val[:int(len(labels_train_and_val) * .80)]\n",
    "labels_val = labels_train_and_val[int(len(labels_train_and_val) * .80):]\n",
    "\n",
    "\n",
    "print(len(images_train))\n",
    "print(len(images_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1000)\n",
      "network/conv-1/kernel:0\n",
      "network/conv-1/bias:0\n",
      "network/conv-2/kernel:0\n",
      "network/conv-2/bias:0\n",
      "network/conv-3/kernel:0\n",
      "network/conv-3/bias:0\n",
      "fc-1/weights:0\n",
      "fc-1/biases:0\n",
      "network/conv-1/kernel/Momentum:0\n",
      "network/conv-1/bias/Momentum:0\n",
      "network/conv-2/kernel/Momentum:0\n",
      "network/conv-2/bias/Momentum:0\n",
      "network/conv-3/kernel/Momentum:0\n",
      "network/conv-3/bias/Momentum:0\n",
      "fc-1/weights/Momentum:0\n",
      "fc-1/biases/Momentum:0\n",
      "Number of trainable variables: 8\n",
      "Total number of variables used 35153\n",
      "ready to test\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 75\n",
    "\n",
    "# Modified from RikHeijdens on https://github.com/tensorflow/tensorflow/issues/6011\n",
    "def spp_layer(image, dimensions=[6, 3, 2, 1]):\n",
    "    # todo: fix this\n",
    "    if tf.less(tf.shape(image)[1], dimensions[0] ** 2) is True:\n",
    "        return None\n",
    "    if tf.less(tf.shape(image)[2], dimensions[0] ** 2) is True:\n",
    "        return None\n",
    "    pool_list = []\n",
    "    for pool_dim in dimensions:\n",
    "        pool_list += max_pool_2d_nxn_regions(image, pool_dim)\n",
    "    return tf.concat(pool_list, axis=1)\n",
    "\n",
    "def max_pool_2d_nxn_regions(inputs, output_size):\n",
    "    inputs_shape = tf.shape(inputs)\n",
    "    h = tf.cast(tf.gather(inputs_shape, 1), tf.int32)\n",
    "    w = tf.cast(tf.gather(inputs_shape, 2), tf.int32)\n",
    "\n",
    "    result = []\n",
    "    n = output_size\n",
    "    for row in range(output_size):\n",
    "        for col in range(output_size):\n",
    "            # start_h = floor(row / n * h)\n",
    "            start_h = tf.cast(tf.floor(tf.multiply(row / n, tf.cast(h, tf.float32))), tf.int32)\n",
    "            # end_h = ceil((row + 1) / n * h)\n",
    "            end_h = tf.cast(tf.ceil(tf.multiply((row + 1) / n, tf.cast(h, tf.float32))), tf.int32)\n",
    "            # start_w = floor(col / n * w)\n",
    "            start_w = tf.cast(tf.floor(tf.multiply(col / n, tf.cast(w, tf.float32))), tf.int32)\n",
    "            # end_w = ceil((col + 1) / n * w)\n",
    "            end_w = tf.cast(tf.ceil(tf.multiply((col + 1) / n, tf.cast(w, tf.float32))), tf.int32)\n",
    "            pooling_region = inputs[:, start_h:end_h, start_w:end_w, :]\n",
    "            pool_result = tf.reduce_max(pooling_region, axis=(1, 2))\n",
    "            result.append(pool_result)\n",
    "    return result\n",
    "\n",
    "\n",
    "TEST_LOGITS_COLLECTION_BCONV = 'TEST_LOGITS_COLLECTION_BCONV'\n",
    "TEST_LOGITS_COLLECTION = 'TEST_LOGITS_COLLECTION'\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    image_placeholders = []\n",
    "    label_placeholders = []\n",
    "\n",
    "    with tf.variable_scope(\"network\") as scope:\n",
    "        training = tf.placeholder_with_default(False, (), name='training')\n",
    "        conv_reuse = None\n",
    "        for i in range(BATCH_SIZE):\n",
    "            # todo: we can add transparent images \n",
    "            image = tf.placeholder(tf.float32, shape=(1,None,None,3), name='image_{}'.format(i))\n",
    "            image_placeholders.append(image)\n",
    "            label = tf.placeholder(tf.int64, shape=(), name='label_{}'.format(i))\n",
    "            label_placeholders.append(label)\n",
    "\n",
    "            logit = tf.to_float(image)\n",
    "            logit = (logit - IMAGES_MEAN) / IMAGES_STD\n",
    "#             tf.add_to_collection(TEST_LOGITS_COLLECTION_BCONV, tf.identity(logit, name='{}_{}'.format(TEST_LOGITS_COLLECTION_BCONV, i)))\n",
    "\n",
    "            logit = tf.layers.conv2d(logit, 20, [1, 1], padding='SAME', reuse=conv_reuse, name='conv-1')\n",
    "            logit = tf.layers.conv2d(logit, 30, [4, 4], padding='SAME', reuse=conv_reuse, name='conv-2')\n",
    "            logit = tf.contrib.layers.max_pool2d(inputs=logit, kernel_size=[2, 2], stride=2, scope='pool-1')\n",
    "            logit = tf.layers.conv2d(logit, 20, [2, 2], padding='SAME', reuse=conv_reuse, name='conv-3')\n",
    "\n",
    "#             layers = [5,15,25]\n",
    "#             for i, d in enumerate(layers):\n",
    "#                 for j in range(len(layers)-1):\n",
    "#                     logit0 = logit\n",
    "#                     logit = tf.layers.conv2d(logit, d, [3, 3], padding='SAME', reuse=conv_reuse, name='conv_{}_{}'.format(i,j))\n",
    "#                     if j>0:\n",
    "#                         logit = logit + logit0\n",
    "#                     \n",
    "#                 logit = tf.contrib.layers.max_pool2d(logit, (3,3), stride=2, scope='pool_{}'.format(i))\n",
    "        \n",
    "            \n",
    "#             tf.add_to_collection(TEST_LOGITS_COLLECTION, tf.identity(logit, name='{}_{}'.format(TEST_LOGITS_COLLECTION, i)))\n",
    "            logit = spp_layer(logit)\n",
    "#             logit = tf.layers.batch_normalization(logit, center=False, scale=False, training=training, reuse=conv_reuse, name='batch_1')\n",
    "            conv_reuse = True\n",
    "            if not logit is None:\n",
    "                logit = tf.reshape(logit, [-1])\n",
    "                tf.add_to_collection(LOGITS_COLLECTION, tf.identity(logit, name='coll_logit_{}'.format(i)))\n",
    "                tf.add_to_collection(LOGIT_LABELS_COLLECTION, tf.identity(label, name='coll_label_{}'.format(i)))\n",
    "\n",
    "            scope.reuse_variables()\n",
    "        \n",
    "#     logits_TEST_BCONV = tf.get_collection(TEST_LOGITS_COLLECTION_BCONV)\n",
    "#     logits_TEST = tf.get_collection(TEST_LOGITS_COLLECTION)\n",
    "    \n",
    "    logits = tf.stack(tf.get_collection(LOGITS_COLLECTION))\n",
    "    logit_labels = tf.stack(tf.get_collection(LOGIT_LABELS_COLLECTION))\n",
    "    print(logits.shape)\n",
    "\n",
    "    logits = tf.layers.dropout(logits, rate=0.3)\n",
    "    logits = tf.contrib.layers.fully_connected(logits, NUM_CLASSES, activation_fn=None, scope=\"fc-1\")\n",
    "\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=logit_labels)) + 1e-6 * tf.losses.get_regularization_loss()\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        opt = tf.train.MomentumOptimizer(0.001, 0.9).minimize(loss)\n",
    "    correct = tf.equal(tf.argmax(logits, -1), logit_labels)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "    [print(v.name) for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)]\n",
    "    print('Number of trainable variables: {}'.format(len(tf.trainable_variables())))\n",
    "    print('Total number of variables used {}'.format(np.sum([v.get_shape().num_elements() for v in tf.trainable_variables()])))\n",
    "    \n",
    "sess = tf.Session(graph=graph)\n",
    "LOG_DIR = 'log/{}'.format(RUN_PREFIX)\n",
    "with graph.as_default(), sess.as_default():\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('ready to test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to test\n",
      "[  0] Accuracy: 0.031  \t  Loss: 3.497  \t  validation accuracy: 0.056\n",
      "[  1] Accuracy: 0.101  \t  Loss: 3.113  \t  validation accuracy: 0.044\n",
      "[  2] Accuracy: 0.125  \t  Loss: 2.986  \t  validation accuracy: 0.087\n",
      "[  3] Accuracy: 0.151  \t  Loss: 2.921  \t  validation accuracy: 0.075\n",
      "[  4] Accuracy: 0.162  \t  Loss: 2.855  \t  validation accuracy: 0.113\n",
      "[  5] Accuracy: 0.197  \t  Loss: 2.794  \t  validation accuracy: 0.163\n",
      "[  6] Accuracy: 0.210  \t  Loss: 2.742  \t  validation accuracy: 0.113\n",
      "[  7] Accuracy: 0.210  \t  Loss: 2.691  \t  validation accuracy: 0.144\n",
      "[  8] Accuracy: 0.235  \t  Loss: 2.637  \t  validation accuracy: 0.169\n",
      "[  9] Accuracy: 0.259  \t  Loss: 2.562  \t  validation accuracy: 0.150\n",
      "[ 10] Accuracy: 0.278  \t  Loss: 2.514  \t  validation accuracy: 0.175\n",
      "[ 11] Accuracy: 0.311  \t  Loss: 2.447  \t  validation accuracy: 0.163\n",
      "[ 12] Accuracy: 0.324  \t  Loss: 2.399  \t  validation accuracy: 0.169\n",
      "[ 13] Accuracy: 0.344  \t  Loss: 2.337  \t  validation accuracy: 0.163\n",
      "[ 14] Accuracy: 0.340  \t  Loss: 2.261  \t  validation accuracy: 0.219\n",
      "[ 15] Accuracy: 0.379  \t  Loss: 2.187  \t  validation accuracy: 0.188\n",
      "[ 16] Accuracy: 0.403  \t  Loss: 2.104  \t  validation accuracy: 0.212\n",
      "[ 17] Accuracy: 0.417  \t  Loss: 2.007  \t  validation accuracy: 0.219\n",
      "[ 18] Accuracy: 0.447  \t  Loss: 1.932  \t  validation accuracy: 0.194\n",
      "[ 19] Accuracy: 0.452  \t  Loss: 1.885  \t  validation accuracy: 0.206\n",
      "[ 20] Accuracy: 0.456  \t  Loss: 1.813  \t  validation accuracy: 0.225\n",
      "[ 21] Accuracy: 0.485  \t  Loss: 1.729  \t  validation accuracy: 0.219\n",
      "[ 22] Accuracy: 0.506  \t  Loss: 1.646  \t  validation accuracy: 0.244\n",
      "[ 23] Accuracy: 0.550  \t  Loss: 1.556  \t  validation accuracy: 0.212\n",
      "[ 24] Accuracy: 0.562  \t  Loss: 1.485  \t  validation accuracy: 0.219\n",
      "[ 25] Accuracy: 0.588  \t  Loss: 1.415  \t  validation accuracy: 0.237\n",
      "[ 26] Accuracy: 0.574  \t  Loss: 1.398  \t  validation accuracy: 0.212\n",
      "[ 27] Accuracy: 0.581  \t  Loss: 1.366  \t  validation accuracy: 0.250\n",
      "[ 28] Accuracy: 0.618  \t  Loss: 1.289  \t  validation accuracy: 0.231\n",
      "[ 29] Accuracy: 0.647  \t  Loss: 1.207  \t  validation accuracy: 0.212\n",
      "[ 30] Accuracy: 0.649  \t  Loss: 1.153  \t  validation accuracy: 0.244\n",
      "[ 31] Accuracy: 0.664  \t  Loss: 1.125  \t  validation accuracy: 0.263\n",
      "[ 32] Accuracy: 0.667  \t  Loss: 1.117  \t  validation accuracy: 0.237\n",
      "[ 33] Accuracy: 0.676  \t  Loss: 1.053  \t  validation accuracy: 0.225\n",
      "[ 34] Accuracy: 0.695  \t  Loss: 1.018  \t  validation accuracy: 0.256\n"
     ]
    }
   ],
   "source": [
    "####################################### PART THREE #######################################\n",
    "print('starting to test')\n",
    "with graph.as_default(), sess.as_default():\n",
    "#     writer = tf.summary.FileWriter(LOG_DIR)\n",
    "#     writer.add_graph(sess.graph)\n",
    "    for epoch in range(EPOCHS):\n",
    "        np.random.seed(epoch)\n",
    "        np.random.shuffle(images_train)\n",
    "        np.random.seed(epoch)\n",
    "        np.random.shuffle(labels_train)\n",
    "        accuracy_vals, loss_vals = [], []\n",
    "        for i in range(0, len(images_train) - BATCH_SIZE + 1, BATCH_SIZE):\n",
    "            batch_images, batch_labels = images_train[i:i + BATCH_SIZE], labels_train[i:i + BATCH_SIZE]\n",
    "        \n",
    "            # todo: this is not very good... (probably replace with 1 x 1 x 1 x 1 when I implement SPP filter, do the same for training)\n",
    "            if BATCH_SIZE - len(batch_images) > 0:\n",
    "#                 print('testing diff: %d'%(BATCH_SIZE - len(batch_images)))\n",
    "                for j in range(len(batch_images), BATCH_SIZE):\n",
    "                    batch_images.append(images_train[j - len(batch_images)])\n",
    "                    batch_labels.append(labels_train[j - len(batch_images)])\n",
    "\n",
    "            fd = {**{k: v for k, v in zip(image_placeholders, batch_images)}, **{k: v for k, v in zip(label_placeholders, batch_labels )}}\n",
    "            fd[training] = True\n",
    "            accuracy_val, loss_val, _ = sess.run([accuracy, loss, opt], feed_dict=fd)\n",
    "            accuracy_vals.append(accuracy_val)\n",
    "            loss_vals.append(loss_val)\n",
    "            \n",
    "#             if i >= BATCH_SIZE * 3:\n",
    "#                 o_logits, o_logits_TEST, o_logits_TEST_BCONV = sess.run([logits, logits_TEST, logits_TEST_BCONV], feed_dict=fd)\n",
    "#                 print('[{}:{}] LOGIT LEN: {} ACC: {} LOSS: {}'.format(epoch, i, len(o_logits), accuracy_val, loss_val))\n",
    "#                 print(o_logits_TEST_BCONV[0])\n",
    "#                 print('[{}:{}] ==========================='.format(epoch, i))\n",
    "#                 print(o_logits_TEST[0])\n",
    "        val_correct = []\n",
    "        for i in range(0, len(images_val), BATCH_SIZE):\n",
    "            batch_images, batch_labels = images_val[i:i + BATCH_SIZE], labels_val[i:i + BATCH_SIZE]\n",
    "            \n",
    "            if BATCH_SIZE - len(batch_images) > 0:\n",
    "#                 print('training diff: %d'%(BATCH_SIZE - len(batch_images)))\n",
    "                for j in range(len(batch_images), BATCH_SIZE):\n",
    "                    batch_images.append(images_val[j - len(batch_images)])\n",
    "                    batch_labels.append(labels_val[j - len(batch_images)])\n",
    "                \n",
    "            fd = {**{k: v for k, v in zip(image_placeholders, batch_images)}, **{k: v for k, v in zip(label_placeholders, batch_labels )}}\n",
    "            c = sess.run(correct, feed_dict=fd)\n",
    "            val_correct.extend(c)\n",
    "\n",
    "        # s = tf.Summary()\n",
    "        # s.value.add(simple_value=np.mean(accuracy_vals), tag=\"accuracy\")\n",
    "        # s.value.add(simple_value=np.mean(loss_vals), tag=\"loss\")\n",
    "        # s.value.add(simple_value=np.mean(val_correct), tag=\"correct\")\n",
    "        # writer.add_summary(s, epoch)\n",
    "        # writer.flush()\n",
    "        # saver.save(sess, path.join(RUN_PREFIX, 'network.ckpt'), global_step=epoch)\n",
    "        print('[%3d] Accuracy: %0.3f  \\t  Loss: %0.3f  \\t  validation accuracy: %0.3f'%(epoch, np.mean(accuracy_vals), np.mean(loss_vals), np.mean(val_correct)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_test, labels_test = load('test.txt')\n",
    "\n",
    "print('Input shape: ' + str(images_test.shape))\n",
    "print('Labels shape: ' + str(labels_test.shape))\n",
    "\n",
    "val_correct = []\n",
    "for i in range(0, images_test.shape[0], BATCH_SIZE):\n",
    "    batch_images, batch_labels = images_test[i:i + BATCH_SIZE], labels_test[i:i + BATCH_SIZE]\n",
    "    fd = {**{k: v for k, v in zip(image_placeholders, batch_images)}, **{k: v for k, v in zip(label_placeholders, batch_labels )}}\n",
    "    val_correct.extend( sess.run(correct, feed_dict=fd) )\n",
    "print(\"ConvNet Validation Accuracy: \", np.mean(val_correct))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.save('birds-{}.tfg'.format(RUN_PREFIX), session=sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
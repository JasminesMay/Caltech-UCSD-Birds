{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "num_classes = 200\n",
    "\n",
    "def load(filename):\n",
    "    file = open(filename, \"r\") \n",
    "    image_names = file.readlines()\n",
    "    images = []\n",
    "    labels = []\n",
    "    for name in image_names:\n",
    "        label = int(name[:3])\n",
    "        if label <= 2:\n",
    "            im = Image.open(\"images/\" + name.rstrip('\\n'))\n",
    "            H, W = im.size\n",
    "            pixels = list(im.getdata())\n",
    "            if not type(pixels[0]) is int:\n",
    "                # todo: right now we are discarding transparent images\n",
    "                image = np.array([comp for pixel in pixels for comp in pixel]).reshape(-1, H, W, 3)\n",
    "                images.append(image)\n",
    "                # zero-index the label\n",
    "                labels.append(label - 1)\n",
    "    return images, labels\n",
    "\n",
    "images_train, labels_train = load('train.txt')\n",
    "# images_test, labels_test = load('test.txt')\n",
    "\n",
    "print(len(images_train))\n",
    "print(len(labels_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8330.9678]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# todo: use tf.contrib.layers.conv2d for 3D filter\n",
    "def conv_relu(input_image, kernel_shape, bias_shape, stride = 2):\n",
    "    strides = [1, stride, stride, 1]\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape, initializer=tf.random_normal_initializer())\n",
    "    biases = tf.get_variable(\"biases\", bias_shape, initializer=tf.constant_initializer(0.0))\n",
    "    conv = tf.nn.conv2d(input_image, weights, strides=strides, padding='SAME')\n",
    "    return tf.nn.relu(conv + biases)\n",
    "\n",
    "def conv_layers(input_image):\n",
    "    # Variables created here will be named \"convX/weights\", \"convX/biases\".\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        out_channels = 15\n",
    "        output = conv_relu(input_image, [5, 5, 3, out_channels], [out_channels], stride=1)\n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        old_out_channels = out_channels\n",
    "        out_channels = 20\n",
    "        output = conv_relu(output, [5, 5, old_out_channels, out_channels], [out_channels])\n",
    "    with tf.variable_scope(\"conv3\"):\n",
    "        old_out_channels = out_channels\n",
    "        out_channels = 3\n",
    "        return conv_relu(output, [5, 5, old_out_channels, out_channels], [out_channels])\n",
    "    \n",
    "# https://github.com/tensorflow/tensorflow/issues/6011\n",
    "# def spp_layer(image, levels=[6, 3, 2, 1]):\n",
    "#     shape = image.get_shape().as_list()\n",
    "#     print(shape)\n",
    "# #     if shape[1] < levels[0] ** 2 or shape[2] < levels[0] ** 2:\n",
    "# #         print(shape)\n",
    "# #         print('Size must be greater than {:d}x{:d}'.format(levels[0], levels[0]))\n",
    "# #         return None\n",
    "\n",
    "#     with tf.variable_scope('spp'):\n",
    "#         pool_outputs = []\n",
    "#         for level in levels:\n",
    "#             # todo: figure out why it is surrounded by 1 (Striding over your images would mean you'd just drop whole images from training or induction.)\n",
    "              # https://stackoverflow.com/questions/43790742/how-to-stride-over-multiple-channels-in-conv2d-of-tensorflow\n",
    "#             window_size = [1] + [np.ceil(d / level).astype(np.int32) for d in shape[1:3]] + [1]\n",
    "#             strides = [1] + [np.floor(d / level + 1).astype(np.int32) for d in shape[1:3]] + [1]\n",
    "            \n",
    "#             pool = tf.nn.max_pool(image, ksize=window_size, strides=strides, padding='SAME')\n",
    "#             pool_outputs.append(tf.reshape(pool, [shape[0], -1]))\n",
    "#         spp_pool = tf.concat(pool_outputs, axis=1)\n",
    "#     return None\n",
    "\n",
    "def max_pool_2d_nxn_regions(inputs, output_size):\n",
    "    inputs_shape = tf.shape(inputs)\n",
    "    h = tf.cast(tf.gather(inputs_shape, 1), tf.int32)\n",
    "    w = tf.cast(tf.gather(inputs_shape, 2), tf.int32)\n",
    "    \n",
    "    pooling_op = tf.reduce_max\n",
    "    \n",
    "#         pooling_op = tf.reduce_mean\n",
    "\n",
    "    result = []\n",
    "    n = output_size\n",
    "    for row in range(output_size):\n",
    "        for col in range(output_size):\n",
    "            # start_h = floor(row / n * h)\n",
    "            start_h = tf.cast(tf.floor(tf.multiply(row / n, tf.cast(h, tf.float32))), tf.int32)\n",
    "            # end_h = ceil((row + 1) / n * h)\n",
    "            end_h = tf.cast(tf.ceil(tf.multiply((row + 1) / n, tf.cast(h, tf.float32))), tf.int32)\n",
    "            # start_w = floor(col / n * w)\n",
    "            start_w = tf.cast(tf.floor(tf.multiply(col / n, tf.cast(w, tf.float32))), tf.int32)\n",
    "            # end_w = ceil((col + 1) / n * w)\n",
    "            end_w = tf.cast(tf.ceil(tf.multiply((col + 1) / n, tf.cast(w, tf.float32))), tf.int32)\n",
    "            pooling_region = inputs[:, start_h:end_h, start_w:end_w, :]\n",
    "            pool_result = pooling_op(pooling_region, axis=(1, 2))\n",
    "            result.append(pool_result)\n",
    "    return result\n",
    "\n",
    "# Modified from RikHeijdens on https://github.com/tensorflow/tensorflow/issues/6011\n",
    "def spp_layer(inputs, dimensions=[3, 2, 1]):\n",
    "    # todo: fix this\n",
    "    # print(inputs.get_shape()[1] < tf.constant(36, dtype=tf.int32))\n",
    "\n",
    "\n",
    "#     if tf.less(inputs.get_shape()[1], dimensions[0] ** 2) or tf.less(inputs.get_shape()[2], dimensions[0] ** 2):\n",
    "#         print(shape)\n",
    "#         print('Size must be greater than {:d}x{:d}'.format(dimensions[0], dimensions[0]))\n",
    "#         return None\n",
    "    pool_list = []\n",
    "    for pool_dim in dimensions:\n",
    "        pool_list += max_pool_2d_nxn_regions(inputs, pool_dim)\n",
    "    return tf.concat(pool_list, axis=1)\n",
    "\n",
    "# todo: might be able to move this into session\n",
    "def fc_layer(image, reuse):\n",
    "    return tf.contrib.layers.fully_connected(image, num_classes, activation_fn=None, scope=\"fc\", reuse=reuse)\n",
    "\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "fc_reuse = False\n",
    "with tf.variable_scope(\"network\") as scope:\n",
    "    image = tf.placeholder(tf.float32, (1,None,None,3), name='image')\n",
    "    h = tf.to_float(image)\n",
    "    h = conv_layers(h)\n",
    "    h = spp_layer(h)\n",
    "    \n",
    "    if not h is None:\n",
    "        h = fc_layer(h, fc_reuse)\n",
    "        h = tf.reshape(h, [-1])\n",
    "        accuracy = tf.reduce_mean(tf.cast(h, tf.float32))\n",
    "#         logits.append(output)\n",
    "#         logit_labels.append(batch_labels[index])\n",
    "        fc_reuse = True\n",
    "\n",
    "    scope.reuse_variables()\n",
    "    \n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "session.run([accuracy], feed_dict={image: images_train[0]})\n",
    "\n",
    "# session.run([h], feed_dict={image: images_train[1]})\n",
    "\n",
    "# print(len(tf.trainable_variables()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "Tensor(\"network/Mean:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Current implementation does not yet support strides in the batch and depth dimensions.\n\t [[Node: network/conv2/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[2, 2, 2, 2], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](network/conv1/Relu, network/conv2/weights/read)]]\n\nCaused by op 'network/conv2/Conv2D', defined at:\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-14-97201657a96f>\", line 25, in <module>\n    output = conv_layers(output)\n  File \"<ipython-input-9-793f1e073674>\", line 17, in conv_layers\n    output = conv_relu(output, [5, 5, old_out_channels, out_channels], [out_channels])\n  File \"<ipython-input-9-793f1e073674>\", line 6, in conv_relu\n    conv = tf.nn.conv2d(input_image, weights, strides=strides, padding='SAME')\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 397, in conv2d\n    data_format=data_format, name=name)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Current implementation does not yet support strides in the batch and depth dimensions.\n\t [[Node: network/conv2/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[2, 2, 2, 2], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](network/conv1/Relu, network/conv2/weights/read)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/Users/michaelscaria/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michaelscaria/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michaelscaria/anaconda/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michaelscaria/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Current implementation does not yet support strides in the batch and depth dimensions.\n\t [[Node: network/conv2/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[2, 2, 2, 2], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](network/conv1/Relu, network/conv2/weights/read)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-97201657a96f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;31m#             print(total_loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michaelscaria/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michaelscaria/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michaelscaria/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/michaelscaria/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Current implementation does not yet support strides in the batch and depth dimensions.\n\t [[Node: network/conv2/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[2, 2, 2, 2], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](network/conv1/Relu, network/conv2/weights/read)]]\n\nCaused by op 'network/conv2/Conv2D', defined at:\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-14-97201657a96f>\", line 25, in <module>\n    output = conv_layers(output)\n  File \"<ipython-input-9-793f1e073674>\", line 17, in conv_layers\n    output = conv_relu(output, [5, 5, old_out_channels, out_channels], [out_channels])\n  File \"<ipython-input-9-793f1e073674>\", line 6, in conv_relu\n    conv = tf.nn.conv2d(input_image, weights, strides=strides, padding='SAME')\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 397, in conv2d\n    data_format=data_format, name=name)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/michaelscaria/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Current implementation does not yet support strides in the batch and depth dimensions.\n\t [[Node: network/conv2/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[2, 2, 2, 2], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](network/conv1/Relu, network/conv2/weights/read)]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "with tf.variable_scope(\"network\") as scope:\n",
    "    fc_reuse = False\n",
    "    for epoch in range(epochs):\n",
    "        np.random.seed(epoch)\n",
    "        np.random.shuffle(images_train)\n",
    "        np.random.seed(epoch)\n",
    "        np.random.shuffle(labels_train)\n",
    "        accuracy_vals, loss_vals = [], []\n",
    "\n",
    "        print(len(images_train))\n",
    "        for i in range(0, len(images_train) - batch_size + 1, batch_size):\n",
    "            batch_images, batch_labels = images_train[i:i + batch_size], labels_train[i:i + batch_size]\n",
    "            logits, logit_labels = [], []\n",
    "        \n",
    "            for index, image in enumerate(batch_images):\n",
    "                output = tf.convert_to_tensor(image)\n",
    "                output = tf.to_float(output)\n",
    "                output = conv_layers(output)\n",
    "                output = spp_layer(output)\n",
    "                if not output is None:\n",
    "                    output = fc_layer(output, fc_reuse)\n",
    "                    output = tf.reshape(output, [-1])\n",
    "                    logits.append(output)\n",
    "                    logit_labels.append(batch_labels[index])\n",
    "                    fc_reuse = True\n",
    "\n",
    "                scope.reuse_variables()\n",
    "            \n",
    "            \n",
    "            loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=logit_labels))\n",
    "            regularization_loss = tf.losses.get_regularization_loss()\n",
    "            total_loss = loss + 1e-6 * regularization_loss\n",
    "            # todo: play around with optimizer\n",
    "#             optimizer = tf.train.MomentumOptimizer(0.001, 0.9)\n",
    "#             with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "#                 opt = optimizer.minimize(total_loss)\n",
    "            correct = tf.equal(tf.argmax(logits, 1), logit_labels)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "            print(session.run([correct]))\n",
    "#             print(total_loss)\n",
    "            \n",
    "#             accuracy_val, loss_val, _ = sess.run([accuracy, total_loss, opt], feed_dict={logits: logits, logits_labels: logits_labels})\n",
    "#             accuracy_vals.append(accuracy_val)\n",
    "#             loss_vals.append(loss_val)\n",
    "\n",
    "#     val_correct = []\n",
    "#     for i in range(0, image_test.shape[0], batch_size):\n",
    "#         batch_images, batch_labels = image_val[i:i + batch_size], label_val[i:i + batch_size]\n",
    "#         val_correct.extend( sess.run(correct, feed_dict={inputs: batch_images, labels: batch_labels}) )\n",
    "#     print('[%3d] Accuracy: %0.3f  \\t  Loss: %0.3f  \\t  validation accuracy: %0.3f'%(epoch, np.mean(accuracy_vals), np.mean(loss_vals), np.mean(val_correct)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/tensorflow/issues/6011\n",
    "# def spp_layer(image, levels=[6, 3, 2, 1]):\n",
    "#     shape = image.get_shape().as_list()\n",
    "#     print(shape)\n",
    "#     if shape[1] < levels[0] ** 2 or shape[2] < levels[0] ** 2:\n",
    "#         print(shape)\n",
    "#         print('Size must be greater than {:d}x{:d}'.format(levels[0], levels[0]))\n",
    "#         return None\n",
    "\n",
    "#     with tf.variable_scope('spp'):\n",
    "#         pool_outputs = []\n",
    "#         for level in levels:\n",
    "#             # todo: figure out why it is surrounded by 1\n",
    "# #             window_size = [1] + [tf.cast(tf.ceil(d / level), tf.int32) for d in tf.shape(image)[1:3]] + [1]\n",
    "# #             strides = [1] + [tf.cast(tf.floor(d / level + 1), tf.int32) for d in tf.shape(image)[1:3]] + [1]\n",
    "# #             win_size = [1, \n",
    "# #                         tf.cast(tf.ceil(tf.shape(image)[1]  / level), tf.int32), \n",
    "# #                         tf.cast(tf.ceil(tf.shape(image)[2]  / level), tf.int32), \n",
    "# #                         1]\n",
    "# #             strides = [1, \n",
    "# #                         tf.cast(tf.floor(tf.shape(image)[1] / level + 1), tf.int32), \n",
    "# #                         tf.cast(tf.floor(tf.shape(image)[2] / level + 1), tf.int32), \n",
    "# #                         1]\n",
    "# #             print(tf.Session().run([image.get_shape()[1]]))\n",
    "#             b = tf.shape(image)[1]\n",
    "#             print(b)\n",
    "#             print(b.eval(tf.Session()))\n",
    "#             window_size = np.ceil(b / level).astype(np.int32)\n",
    "#             print(window_size)\n",
    "            \n",
    "# #             pool = tf.nn.max_pool(image, ksize=window_size, strides=strides, padding='SAME')\n",
    "# #             pool_outputs.append(tf.reshape(pool, [shape[0], -1]))\n",
    "# #         spp_pool = tf.concat(pool_outputs, axis=1)\n",
    "#     return None\n",
    "\n",
    "# def spp_layer(input_, levels=[3, 2, 1], name = 'SPP_layer'):\n",
    "#     '''Multiple Level SPP layer.\n",
    "#        Works for levels=[1, 2, 3, 6].'''\n",
    "    \n",
    "#     shape = input_.get_shape().as_list()\n",
    "#     print(shape)\n",
    "#     with tf.variable_scope(name):\n",
    "#         pool_outputs = []\n",
    "#         for l in levels:\n",
    "#             p = [1, tf.cast(tf.ceil(tf.shape(input_)[1]  / l), tf.int32), tf.cast(tf.ceil(tf.shape(input_)[2]  / l), tf.int32), 1]\n",
    "#             print(p)\n",
    "#             pool = tf.nn.max_pool(input_, ksize=p, \n",
    "#                                       strides=[1, tf.cast(tf.floor(tf.shape(input_)[1]  / l + 1), tf.int32), tf.floor(tf.shape(input_)[2] / l + 1), 1], \n",
    "#                                       padding='SAME')\n",
    "# #             print \"Pool Level {:}: shape {:}\".format(l, pool.get_shape().as_list())\n",
    "#             pool_outputs.append(tf.reshape(pool, [tf.shape(input_)[0], -1]))\n",
    "#         spp_pool = tf.concat(pool_outputs, axis=1)\n",
    "#     return spp_pool\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
